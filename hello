#include <pthread.h>
#include <stdlib.h>
#include <stdio.h>

void* comp(void *arg) {
    printf("Computation\n");
    int num = *((int *)arg);
    printf("Args: %ls", &num);

    return NULL;
}


int main(int argc, char const *argv[])
{
    pthread_t thread1;
    int *num = (int *)234;
    pthread_create(&thread1, NULL, comp, num);
    pthread_join(thread1, NULL);
    return 0;
}


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

#include <stdio.h>
#include <stdlib.h>
#include <pthread.h>

#define ROWS 5
#define COLS 5
#define NUM_THREADS 2

int A[ROWS][COLS], B[ROWS][COLS], C[ROWS][COLS];
pthread_mutex_t lock;  // Mutex lock for thread synchronization

// Function for matrix addition (executed by each thread)
void *add_matrices(void *arg) {
    int thread_id = *(int *)arg;

    int start_row = (thread_id * ROWS) / NUM_THREADS;
    int end_row = ((thread_id + 1) * ROWS) / NUM_THREADS;

    for (int i = start_row; i < end_row; i++) {
        for (int j = 0; j < COLS; j++) {
            pthread_mutex_lock(&lock);  // Lock before updating shared matrix
            C[i][j] = A[i][j] + B[i][j];
            pthread_mutex_unlock(&lock);  // Unlock after updating
        }
    }
    return NULL;
}

// Function to print a matrix
void print_matrix(int matrix[ROWS][COLS]) {
    for (int i = 0; i < ROWS; i++) {
        for (int j = 0; j < COLS; j++) {
            printf("%d ", matrix[i][j]);
        }
        printf("\n");
    }
}

int main() {
    pthread_t threads[NUM_THREADS];
    pthread_mutex_init(&lock, NULL);  // Initialize mutex lock

    // Initialize matrices A and B with random values
    for (int i = 0; i < ROWS; i++) {
        for (int j = 0; j < COLS; j++) {
            A[i][j] = rand() % 10;
            B[i][j] = rand() % 10;
        }
    }

    printf("Matrix A:\n");
    print_matrix(A);
    printf("Matrix B:\n");
    print_matrix(B);

    for (int i = 0; i < NUM_THREADS; i++) {
        int *thread_id = malloc(sizeof(int));
        *thread_id = i;
        pthread_create(&threads[i], NULL, add_matrices, thread_id);
    }

    // Wait for all threads to complete
    for (int i = 0; i < NUM_THREADS; i++) {
        pthread_join(threads[i], NULL);
    }

    pthread_mutex_destroy(&lock);  // Clean up the mutex

    printf("Resultant Matrix (A + B):\n");
    print_matrix(C);

    return 0;
}

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

#include <stdio.h>
#include <stdlib.h>
#include <pthread.h>

#define ROWS 5
#define COLS 5
#define NUM_THREADS 2

int A[ROWS][COLS], B[COLS][ROWS], C[ROWS][ROWS];
pthread_mutex_t lock;  // Mutex lock for thread synchronization

// Function for matrix multiplication (executed by each thread)
void *multiply_matrices(void *arg) {
    int thread_id = *(int *)arg;

    int start_row = (thread_id * ROWS) / NUM_THREADS;
    int end_row = ((thread_id + 1) * ROWS) / NUM_THREADS;

    for (int i = start_row; i < end_row; i++) {
        for (int j = 0; j < ROWS; j++) {
            int sum = 0;
            for (int k = 0; k < COLS; k++) {
                sum += A[i][k] * B[k][j];
            }
            pthread_mutex_lock(&lock);  // Lock before updating shared matrix
            C[i][j] = sum;
            pthread_mutex_unlock(&lock);  // Unlock after updating
        }
    }
    return NULL;
}

// Function to print a matrix
void print_matrix(int matrix[ROWS][ROWS]) {
    for (int i = 0; i < ROWS; i++) {
        for (int j = 0; j < ROWS; j++) {
            printf("%d ", matrix[i][j]);
        }
        printf("\n");
    }
}

int main() {
    pthread_t threads[NUM_THREADS];
    pthread_mutex_init(&lock, NULL);  // Initialize mutex lock

    // Initialize matrices A and B with random values
    for (int i = 0; i < ROWS; i++) {
        for (int j = 0; j < COLS; j++) {
            A[i][j] = rand() % 10;
            B[j][i] = rand() % 10; // Ensure B is transposed for multiplication
        }
    }

    printf("Matrix A:\n");
    print_matrix(A);
    printf("Matrix B:\n");
    print_matrix(B);

    for (int i = 0; i < NUM_THREADS; i++) {
        int *thread_id = malloc(sizeof(int));
        *thread_id = i;
        pthread_create(&threads[i], NULL, multiply_matrices, thread_id);
    }

    // Wait for all threads to complete
    for (int i = 0; i < NUM_THREADS; i++) {
        pthread_join(threads[i], NULL);
    }

    pthread_mutex_destroy(&lock);  // Clean up the mutex

    printf("Resultant Matrix (A * B):\n");
    print_matrix(C);

    return 0;
}

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

#include<stdlib.h>
#include<stdio.h>
#include<omp.h>
int main(int argc, char const *argv[])
{
int A[10] = {1,2,3,4,5,6,7,8,9,10};
int B[10] = {1,2,3,4,5,6,7,8,9,10};
int C[10];
# pragma omp parallel
{
int thread_id = omp_get_thread_num();
int num_threads = omp_get_num_threads();
print("Thread %d of %d is executing \n", thread_id, num_threads);
for (int i=0; i<10; i++)
{
C[i]= A[i]+B[i];
}
printf("The new values are:");
for (int j=0;j<10;j++)
{
printf(" %d", C[j]);
}
print("\n");
}
return 0;
}

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

#include <stdio.h>
#include <time.h>
#include <stdlib.h>
#include <omp.h>

#define SIZE 500000

int main() {
    int A[SIZE], B[SIZE], C[SIZE];

    // Initialize arrays with random values
    for (int i = 0; i < SIZE; i++) {
        A[i] = rand() % 100;
        B[i] = rand() % 10;
    }

    /*** Sequential Code ***/
    clock_t begin = clock();
    for (int i = 0; i < SIZE; i++) {
        C[i] = A[i] * B[i];
    }
    clock_t end = clock();
    double time_sequential = (double)(end - begin) / CLOCKS_PER_SEC;
    printf("Sequential Time: %f seconds\n\n", time_sequential);

    // Set the number of OpenMP threads
    omp_set_num_threads(8);

    /*** OpenMP Parallel Region (Without Data Distribution) ***/
    double start_time_parallel = omp_get_wtime();
    #pragma omp parallel
    {
        for (int i = 0; i < SIZE; i++) {
            C[i] = A[i] * B[i];
        }
    }
    double time_parallel = omp_get_wtime() - start_time_parallel;
    printf("Parallel (Without Data Distribution) Time: %f seconds\n\n", time_parallel);

    /*** OpenMP Parallel Region (With Manual Data Distribution) ***/
    double start_time_manual = omp_get_wtime();
    #pragma omp parallel
    {
        int thread_id = omp_get_thread_num();
        int thread_count = omp_get_num_threads();

        // Calculate start and end indices for each thread
        int start_index = thread_id * SIZE / thread_count;
        int end_index = (thread_id + 1) * SIZE / thread_count;

        // Ensure the last thread covers the remaining elements
        if (thread_id == thread_count - 1) {
            end_index = SIZE;
        }

        for (int i = start_index; i < end_index; i++) {
            C[i] = A[i] * B[i];
        }
    }
    double time_manual = omp_get_wtime() - start_time_manual;
    printf("Parallel (With Manual Data Distribution) Time: %f seconds\n\n", time_manual);

    /*** OpenMP Parallel Region (Using Loop Construct) ***/
    double start_time_loop = omp_get_wtime();
    #pragma omp parallel
    {
        #pragma omp for
        for (int i = 0; i < SIZE; i++) {
            C[i] = A[i] * B[i];
        }
    }
    double time_loop = omp_get_wtime() - start_time_loop;
    printf("Parallel (Using Loop Construct) Time: %f seconds\n", time_loop);

    return 0;
}

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

#include <stdio.h>
#include <stdlib.h>
#include <omp.h>

// Recursive serial summation function
int summation(int *array, int N) {
    if (N == 0) return 0;           // Base case: Empty array
    else if (N == 1) return *array; // Base case: Single element
    
    int half = N / 2;               // Divide array into two halves
    
    // Recursive summation of left and right halves
    return summation(array, half) + summation(array + half, N - half);
}

// Parallel summation using OpenMP tasks
int summation_parallel(int *A, int N) {
    if (N == 0) return 0;
    else if (N == 1) return *A;

    int half = N / 2;
    int x = 0, y = 0;

    // Set the number of threads
    omp_set_num_threads(4);

    #pragma omp parallel
    {
        #pragma omp single  // Ensure only one thread creates tasks
        {
            #pragma omp task shared(x)
            x = summation_parallel(A, half);  // Left half task

            #pragma omp task shared(y)
            y = summation_parallel(A + half, N - half); // Right half task

            #pragma omp taskwait // Wait for both tasks to finish
            x += y;              // Combine results
        }
    }
    return x;
}

int main() {
    int sz = 500;
    int A[sz];
    
    // Initialize array with random values
    for (int i = 0; i < sz; i++) {
        A[i] = rand() % 10;
    }

    // Serial summation
    printf("Serial Sum = %d\n", summation(A, sz));

    // Parallel summation
    printf("Parallel Sum = %d\n", summation_parallel(A, sz));

    return 0;
}

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

#include <stdio.h>
#include <omp.h>

int main() {
    int i, n = 8;         // 'n' is the number to compute factorial
    int fac = 1;          // Initialize factorial result

    // Set the number of threads to 4
    omp_set_num_threads(4);

    // Parallelize the loop with OpenMP
    #pragma omp parallel for ordered shared(n) private(i) reduction(*:fac)
    for (i = 1; i <= n; i++) {
        fac *= i;  // Multiply current value of i

        // Ordered ensures the output order is maintained
        #pragma omp ordered
        printf("Thread %d - iteration %d - fac(%d) = %d\n",
               omp_get_thread_num(), i, n, fac);
    }

    // Print the final factorial value
    printf("Factorial of %d = %d\n", n, fac);
    return 0;
}

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

#include <iostream>
#include <cuda_runtime.h>

// CUDA kernel to multiply a row vector with a column vector
__global__ void vectorMultiply(int *rowVector, int *colVector, int *result, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i == 0) {
        result[0] = 0; // Initialize result to 0
        for (int j = 0; j < n; ++j) {
            result[0] += rowVector[j] * colVector[j]; // Dot product calculation
        }
    }
}

int main() {
    int numRows, numCols;

    // Get the size of the row and column vectors from the user
    std::cout << "Enter the number of elements in the row vector: ";
    std::cin >> numRows;

    std::cout << "Enter the number of elements in the column vector: ";
    std::cin >> numCols;

    // Input validation
    if (numRows <= 0 || numCols <= 0) {
        std::cerr << "Error: Vector dimensions must be greater than 0.\n";
        return 1;
    }

    if (numRows != numCols) {
        std::cerr << "Error: Dimensions must be equal for dot product.\n";
        return 1;
    }

    // Host vectors
    int *h_rowVector = new int[numRows];
    int *h_colVector = new int[numCols];
    int *h_result = new int[1];

    // Get values for row and column vectors
    std::cout << "Enter values for the row vector:\n";
    for (int i = 0; i < numRows; ++i) {
        std::cout << "Value " << i + 1 << ": ";
        std::cin >> h_rowVector[i];
    }

    std::cout << "Enter values for the column vector:\n";
    for (int i = 0; i < numCols; ++i) {
        std::cout << "Value " << i + 1 << ": ";
        std::cin >> h_colVector[i];
    }

    // Device vectors
    int *d_rowVector, *d_colVector, *d_result;

    // Allocate memory on the device
    cudaMalloc((void**)&d_rowVector, numRows * sizeof(int));
    cudaMalloc((void**)&d_colVector, numCols * sizeof(int));
    cudaMalloc((void**)&d_result, sizeof(int));

    // Copy vectors from host to device
    cudaMemcpy(d_rowVector, h_rowVector, numRows * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_colVector, h_colVector, numCols * sizeof(int), cudaMemcpyHostToDevice);

    // Define block and grid sizes (1 block because the computation is small)
    dim3 blockSize(256);
    dim3 gridSize(1);

    // Launch the kernel
    vectorMultiply<<<gridSize, blockSize>>>(d_rowVector, d_colVector, d_result, numRows);

    // Copy the result from device to host
    cudaMemcpy(h_result, d_result, sizeof(int), cudaMemcpyDeviceToHost);

    // Output the result
    std::cout << "Resultant Scalar (Dot Product): " << h_result[0] << "\n";

    // Free memory on the device
    cudaFree(d_rowVector);
    cudaFree(d_colVector);
    cudaFree(d_result);

    // Free memory on the host
    delete[] h_rowVector;
    delete[] h_colVector;
    delete[] h_result;

    return 0;
}

